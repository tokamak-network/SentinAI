# ==========================================
# SentinAI Configuration
# Run 'npm run setup' for guided setup.
# ==========================================

# === REQUIRED ===
L2_RPC_URL=https://your-l2-rpc-endpoint.com

# === AI Provider (하나 이상 선택) ===
# Option 1: Qwen (우선순위 1번, OpenAI 호환)
QWEN_API_KEY=your-qwen-api-key-here
# QWEN_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode  # 기본값: DashScope. 자체 호스팅 시 변경
# QWEN_MODEL=qwen-turbo-latest                                  # 기본값: fast=qwen-turbo-latest, best=qwen-max-latest
# Option 2: Anthropic Direct API
# ANTHROPIC_API_KEY=sk-ant-...
# Option 3: OpenAI / LiteLLM (OpenAI 호환)
# OPENAI_API_KEY=sk-...
# OPENAI_BASE_URL=http://localhost:4000             # LiteLLM 프록시 사용 시. 미설정 시 api.openai.com
# OPENAI_MODEL=qwen/qwen-turbo-latest               # LiteLLM 모델명 (fast/best 공통). 미설정 시 gpt-4.1-mini / gpt-4.1
# OPENAI_MODEL_FAST=qwen3-coder-flash               # fast 티어 전용 (OPENAI_MODEL보다 우선)
# OPENAI_MODEL_BEST=qwen3-235b                      # best 티어 전용 (OPENAI_MODEL보다 우선)
# Option 4: Google Gemini Direct API
# GEMINI_API_KEY=AIza...

# === Module-specific AI Providers (하이브리드 전략, 선택사항) ===
# 각 모듈별로 AI 제공자를 지정합니다. 설정하지 않으면 기본 제공자 사용.
#
# 권장 설정 (비용 75% 절감 + 90% 성능):
# - ANOMALY_PROVIDER=gemini      (Gemini 2.0 Flash 사용, 초고속)
# - COST_PROVIDER=anthropic      (Claude Sonnet 사용, 복잡 분석)
# - RCA_PROVIDER=anthropic       (Claude Sonnet 사용, 근본 원인)
# - REPORT_PROVIDER=anthropic    (Claude Opus 사용, 고품질 보고서)
# - PREDICTOR_PROVIDER=gemini    (Gemini 2.0 Flash 사용, 빠른 예측)
# - LOG_ANALYZER_PROVIDER=gemini (Gemini 2.0 Flash 사용, 빠른 분석)
#
# 사용 가능한 값: anthropic, openai, gemini, litellm
#
# ANOMALY_PROVIDER=gemini
# COST_PROVIDER=anthropic
# RCA_PROVIDER=anthropic
# REPORT_PROVIDER=anthropic
# PREDICTOR_PROVIDER=gemini
# LOG_ANALYZER_PROVIDER=gemini

# === K8s Monitoring (Optional) ===
# Set AWS_CLUSTER_NAME only — K8S_API_URL and region are auto-detected.
# AWS auth: use 'aws configure' or IAM Role (no credentials needed in .env)
AWS_CLUSTER_NAME=my-cluster-name
# K8S_NAMESPACE=default
# K8S_APP_PREFIX=op

# === State Store (Optional) ===
# Enables Redis-backed state persistence. If not set, uses in-memory (resets on restart).
# REDIS_URL=redis://localhost:6379

# === Alert (Optional) ===
# ALERT_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL   # 이상 탐지 알림 웹훅

# === Cost Tracking (Optional) ===
# COST_TRACKING_ENABLED=true   # vCPU 사용 패턴 추적 (기본: 활성화, false로 비활성화)

# === Cloudflare Tunnel (Optional) ===
# HTTPS + 인증으로 대시보드 공개. docker compose --profile tunnel up -d 로 활성화.
# 설정: https://one.dash.cloudflare.com → Networks → Tunnels → Token 복사
# CLOUDFLARE_TUNNEL_TOKEN=eyJhIjoiYWJj...

# === Agent Loop (Optional) ===
# AGENT_LOOP_ENABLED=true    # Server-side autonomous loop (auto-enabled if L2_RPC_URL is set)
# AUTO_REMEDIATION_ENABLED=true  # Layer 4 auto-remediation (disabled by default)

# === Advanced (usually not needed) ===
# AI_GATEWAY_URL=https://api.ai.tokamak.network  # LiteLLM Gateway (설정 시 직접 API 대신 Gateway 사용)
# K8S_API_URL=https://...    # Override auto-detection
# K8S_INSECURE_TLS=true      # Skip TLS verification (dev/self-signed certs only)
