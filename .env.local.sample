# ==========================================
# SentinAI Configuration
# Run 'npm run setup' for guided setup.
# ==========================================

# === REQUIRED ===
L2_RPC_URL=https://your-l2-rpc-endpoint.com

# === AI Provider (하나 이상 선택) ===
# Option 1: Qwen (우선순위 1번, OpenAI 호환)
QWEN_API_KEY=your-qwen-api-key-here
# QWEN_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode  # 기본값: DashScope. 자체 호스팅 시 변경
# QWEN_MODEL=qwen-turbo-latest                                  # 기본값: fast=qwen-turbo-latest, best=qwen-max-latest
# Option 2: Anthropic Direct API
# ANTHROPIC_API_KEY=sk-ant-...
# Option 3: OpenAI / LiteLLM (OpenAI 호환)
# OPENAI_API_KEY=sk-...
# OPENAI_BASE_URL=http://localhost:4000             # LiteLLM 프록시 사용 시. 미설정 시 api.openai.com
# OPENAI_MODEL=qwen/qwen-turbo-latest               # LiteLLM 모델명 (fast/best 공통). 미설정 시 gpt-4.1-mini / gpt-4.1
# OPENAI_MODEL_FAST=qwen3-coder-flash               # fast 티어 전용 (OPENAI_MODEL보다 우선)
# OPENAI_MODEL_BEST=qwen3-235b                      # best 티어 전용 (OPENAI_MODEL보다 우선)
# Option 4: Google Gemini Direct API
# GEMINI_API_KEY=AIza...

# === Module-specific AI Providers (하이브리드 전략, 선택사항) ===
# 각 모듈별로 AI 제공자를 지정합니다. 설정하지 않으면 기본 제공자 사용.
#
# 권장 설정 (비용 75% 절감 + 90% 성능):
# - ANOMALY_PROVIDER=gemini      (Gemini 2.0 Flash 사용, 초고속)
# - COST_PROVIDER=anthropic      (Claude Sonnet 사용, 복잡 분석)
# - RCA_PROVIDER=anthropic       (Claude Sonnet 사용, 근본 원인)
# - REPORT_PROVIDER=anthropic    (Claude Opus 사용, 고품질 보고서)
# - PREDICTOR_PROVIDER=gemini    (Gemini 2.0 Flash 사용, 빠른 예측)
# - LOG_ANALYZER_PROVIDER=gemini (Gemini 2.0 Flash 사용, 빠른 분석)
#
# 사용 가능한 값: anthropic, openai, gemini, litellm
#
# ANOMALY_PROVIDER=gemini
# COST_PROVIDER=anthropic
# RCA_PROVIDER=anthropic
# REPORT_PROVIDER=anthropic
# PREDICTOR_PROVIDER=gemini
# LOG_ANALYZER_PROVIDER=gemini

# === K8s Monitoring (Optional) ===
# Set AWS_CLUSTER_NAME only — K8S_API_URL and region are auto-detected.
# AWS auth: use 'aws configure' or IAM Role (no credentials needed in .env)
AWS_CLUSTER_NAME=my-cluster-name
# K8S_NAMESPACE=default
# K8S_APP_PREFIX=op

# === State Store (Optional) ===
# Enables Redis-backed state persistence. If not set, uses in-memory (resets on restart).
# REDIS_URL=redis://localhost:6379

# === Alert (Optional) ===
# ALERT_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL   # 이상 탐지 알림 웹훅

# === Cost Tracking (Optional) ===
# COST_TRACKING_ENABLED=true   # vCPU 사용 패턴 추적 (기본: 활성화, false로 비활성화)

# === Cloudflare Tunnel (Optional) ===
# HTTPS + 인증으로 대시보드 공개. docker compose --profile tunnel up -d 로 활성화.
# 설정: https://one.dash.cloudflare.com → Networks → Tunnels → Token 복사
# CLOUDFLARE_TUNNEL_TOKEN=eyJhIjoiYWJj...

# === Agent Loop (Optional) ===
# AGENT_LOOP_ENABLED=true    # Server-side autonomous loop (auto-enabled if L2_RPC_URL is set)
# AUTO_REMEDIATION_ENABLED=true  # Layer 4 auto-remediation (disabled by default)

# === EOA Balance Monitoring (Optional) ===
# Monitor batcher/proposer L1 ETH balances and auto-refill from treasury.
#
# EOA Address Configuration (priority order):
# 1. Manual EOA addresses (if you know the addresses)
#    BATCHER_EOA_ADDRESS=0x...     # Batcher EOA address to monitor
#    PROPOSER_EOA_ADDRESS=0x...    # Proposer EOA address to monitor
#
# 2. Private key derivation (derive EOA from private key, no L1 RPC required)
#    BATCHER_PRIVATE_KEY=0x...     # Batcher private key → derive EOA address
#    PROPOSER_PRIVATE_KEY=0x...    # Proposer private key → derive EOA address
#
# Auto-refill Configuration (requires treasury wallet):
# TREASURY_PRIVATE_KEY=0x...    # Treasury wallet private key for auto-refill (omit for monitor-only)
# EOA_BALANCE_WARNING_ETH=0.5   # Warning threshold (default: 0.5 ETH)
# EOA_BALANCE_CRITICAL_ETH=0.1  # Critical threshold — triggers auto-refill (default: 0.1 ETH)
# EOA_BALANCE_EMERGENCY_ETH=0.01  # Emergency threshold — immediate escalation (default: 0.01 ETH)
# EOA_REFILL_AMOUNT_ETH=0.5     # Amount to send per refill (default: 0.5 ETH)
# EOA_MAX_DAILY_REFILL_ETH=5    # Daily refill cap (default: 5 ETH)
# EOA_COOLDOWN_MINUTES=10       # Cooldown between refills per EOA (default: 10 min)
# EOA_GAS_GUARD_GWEI=100        # Skip refill if L1 gas > this (default: 100 gwei)

# === L1 RPC for SentinAI (Optional) ===
# SentinAI monitoring & analytics use public L1 RPC endpoints.
# L2 nodes (op-node, op-batcher, op-proposer) use SEPARATE L1 RPC via Proxyd (see proxyd-failover-setup.md).
#
# For SentinAI: Use comma-separated public RPC endpoints with automatic failover.
# Falls back to L1_RPC_URL if set, then publicnode.com as last resort.
# Examples with public RPC endpoints:
# L1_RPC_URLS=https://ethereum-sepolia-rpc.publicnode.com,https://sepolia.drpc.org,https://rpc.sepolia.org
# L1_RPC_URL=https://ethereum-sepolia-rpc.publicnode.com  # Primary RPC (if L1_RPC_URLS not set)
#
# For L2 nodes: Configure via L1 Proxyd (separate from SentinAI)
# See docs/guide/proxyd-failover-setup.md for Proxyd configuration
# K8S_STATEFULSET_PREFIX=sepolia-thanos-stack   # StatefulSet name prefix for kubectl set env

# === L1 Proxyd Integration (Optional) ===
# Enable when op-node/batcher/proposer connect to L1 through Proxyd (eth-optimism/infra).
# When enabled, failover updates Proxyd ConfigMap TOML before updating StatefulSet env vars.
# L1_PROXYD_ENABLED=false
# L1_PROXYD_CONFIGMAP_NAME=proxyd-config        # ConfigMap name (default: proxyd-config)
# L1_PROXYD_DATA_KEY=proxyd.toml                 # TOML key in ConfigMap (default: proxyd.toml)
# L1_PROXYD_UPSTREAM_GROUP=main                  # Upstream group to update (default: main)
# L1_PROXYD_UPDATE_MODE=replace                  # replace (update URL) | append (add new + rotate)
# L1_PROXYD_SPARE_URLS=https://spare-rpc1.io,https://spare-rpc2.io  # Spare RPC URLs for 429 backend auto-replacement

# === LLM Stress Test Framework (Optional) ===
# Configure API server endpoints for LLM benchmarking tests.
# If not set, uses default endpoints (OpenAI, DashScope, Anthropic, Google).
# Useful for testing against custom/local API servers or proxies.
#
# Test execution: npx tsx src/lib/__tests__/llm-stress-test/index.ts
# Output: src/lib/__tests__/llm-stress-test/output/report-*.md (Markdown) + results-*.json (JSON)
#
# API Server Configuration (custom endpoints):
# LLM_TEST_QWEN_URL=https://dashscope.aliyuncs.com/compatible-mode  # Qwen API (OpenAI compatible)
# LLM_TEST_ANTHROPIC_URL=https://api.anthropic.com                  # Anthropic API
# LLM_TEST_OPENAI_URL=https://api.openai.com                        # OpenAI API
# LLM_TEST_GEMINI_URL=https://generativelanguage.googleapis.com      # Gemini API
#
# Proxy Configuration (LiteLLM Gateway or similar):
# LLM_TEST_PROXY_URL=http://localhost:4000                          # LiteLLM Gateway or custom proxy
# LLM_TEST_PROXY_ENABLED=false                                      # Route all requests through proxy
#
# Test Execution Configuration:
# LLM_TEST_PROVIDERS=qwen,anthropic,openai,gemini                   # Providers to test (comma-separated)
# LLM_TEST_TIMEOUT_FAST=30000                                       # Fast-tier request timeout (ms)
# LLM_TEST_TIMEOUT_BEST=60000                                       # Best-tier request timeout (ms)
# LLM_TEST_PARALLELISM_DEFAULT=5                                    # Default concurrent requests
# LLM_TEST_OUTPUT_DIR=src/lib/__tests__/llm-stress-test/output      # Result output directory

# === Advanced (usually not needed) ===
# AI_GATEWAY_URL=https://api.ai.tokamak.network  # LiteLLM Gateway (설정 시 직접 API 대신 Gateway 사용)
# K8S_API_URL=https://...    # Override auto-detection
# K8S_INSECURE_TLS=true      # Skip TLS verification (dev/self-signed certs only)
