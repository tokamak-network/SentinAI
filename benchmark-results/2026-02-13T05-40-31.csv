prompt_id,model_id,provider,tier,iteration,latency_ms,tokens_in,tokens_out,cost_usd,accuracy,error
predictive-scaler,qwen3-coder-flash,qwen,fast,1,4561,1103,216,0.000660,1,
predictive-scaler,gpt-5.2,openai,fast,1,5432,971,263,0.026400,1,
predictive-scaler,qwen3-235b,qwen,best,1,5913,1103,250,0.006765,0,
predictive-scaler,gpt-5.2-pro,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.UnsupportedParamsError: gpt-5 models (including gpt-5-codex) don't support temperature=0.2. Only temperature=1 is supported. For gpt-5.1, temperature is supported when reasoning_effort='none' (or not specified, as it defaults to 'none'). To drop unsupported params set `litellm.drop_params = True`. Received Model Group=gpt-5.2-pro\nAvailable Model Group Fallbacks=None"",""type"":""None"",""param"":null,""code"":""400""}}"
predictive-scaler,qwen3-80b-next,qwen,best,1,2254,1103,260,0.004058,1,
predictive-scaler,gpt-5.2-codex,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.BadRequestError: OpenAIException - {\n  \""error\"": {\n    \""message\"": \""Unsupported parameter: 'temperature' is not supported with this model.\"",\n    \""type\"": \""invalid_request_error\"",\n    \""param\"": \""temperature\"",\n    \""code\"": null\n  }\n}. Received Model Group=gpt-5.2-codex\nAvailable Model Group Fallbacks=None"",""type"":null,""param"":null,""code"":""400""}}"
anomaly-analyzer,qwen3-coder-flash,qwen,fast,1,3875,830,263,0.000547,1,
anomaly-analyzer,gpt-5.2,openai,fast,1,11006,760,532,0.035340,1,
anomaly-analyzer,qwen3-235b,qwen,best,1,4618,830,198,0.005140,1,
anomaly-analyzer,gpt-5.2-pro,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.UnsupportedParamsError: gpt-5 models (including gpt-5-codex) don't support temperature=0.2. Only temperature=1 is supported. For gpt-5.1, temperature is supported when reasoning_effort='none' (or not specified, as it defaults to 'none'). To drop unsupported params set `litellm.drop_params = True`. Received Model Group=gpt-5.2-pro\nAvailable Model Group Fallbacks=None"",""type"":""None"",""param"":null,""code"":""400""}}"
anomaly-analyzer,qwen3-80b-next,qwen,best,1,1995,830,232,0.003235,1,
anomaly-analyzer,gpt-5.2-codex,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.BadRequestError: OpenAIException - {\n  \""error\"": {\n    \""message\"": \""Unsupported parameter: 'temperature' is not supported with this model.\"",\n    \""type\"": \""invalid_request_error\"",\n    \""param\"": \""temperature\"",\n    \""code\"": null\n  }\n}. Received Model Group=gpt-5.2-codex\nAvailable Model Group Fallbacks=None"",""type"":null,""param"":null,""code"":""400""}}"
rca-engine,qwen3-coder-flash,qwen,fast,1,7622,1377,535,0.000956,1,
rca-engine,gpt-5.2,openai,fast,1,11668,1303,610,0.046995,1,
rca-engine,qwen3-235b,qwen,best,1,10625,1377,467,0.009220,1,
rca-engine,gpt-5.2-pro,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.UnsupportedParamsError: gpt-5 models (including gpt-5-codex) don't support temperature=0.2. Only temperature=1 is supported. For gpt-5.1, temperature is supported when reasoning_effort='none' (or not specified, as it defaults to 'none'). To drop unsupported params set `litellm.drop_params = True`. Received Model Group=gpt-5.2-pro\nAvailable Model Group Fallbacks=None"",""type"":""None"",""param"":null,""code"":""400""}}"
rca-engine,qwen3-80b-next,qwen,best,1,3632,1377,496,0.005922,1,
rca-engine,gpt-5.2-codex,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.BadRequestError: OpenAIException - {\n  \""error\"": {\n    \""message\"": \""Unsupported parameter: 'temperature' is not supported with this model.\"",\n    \""type\"": \""invalid_request_error\"",\n    \""param\"": \""temperature\"",\n    \""code\"": null\n  }\n}. Received Model Group=gpt-5.2-codex\nAvailable Model Group Fallbacks=None"",""type"":null,""param"":null,""code"":""400""}}"
daily-report,qwen3-coder-flash,qwen,fast,1,0,0,0,0.000000,0,"This operation was aborted"
daily-report,gpt-5.2,openai,fast,1,0,0,0,0.000000,0,"This operation was aborted"
daily-report,qwen3-235b,qwen,best,1,47727,930,2063,0.014965,1,
daily-report,gpt-5.2-pro,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.UnsupportedParamsError: gpt-5 models (including gpt-5-codex) don't support temperature=0.2. Only temperature=1 is supported. For gpt-5.1, temperature is supported when reasoning_effort='none' (or not specified, as it defaults to 'none'). To drop unsupported params set `litellm.drop_params = True`. Received Model Group=gpt-5.2-pro\nAvailable Model Group Fallbacks=None"",""type"":""None"",""param"":null,""code"":""400""}}"
daily-report,qwen3-80b-next,qwen,best,1,16658,930,2297,0.013810,1,
daily-report,gpt-5.2-codex,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.BadRequestError: OpenAIException - {\n  \""error\"": {\n    \""message\"": \""Unsupported parameter: 'temperature' is not supported with this model.\"",\n    \""type\"": \""invalid_request_error\"",\n    \""param\"": \""temperature\"",\n    \""code\"": null\n  }\n}. Received Model Group=gpt-5.2-codex\nAvailable Model Group Fallbacks=None"",""type"":null,""param"":null,""code"":""400""}}"
nlops-responder,qwen3-coder-flash,qwen,fast,1,3459,457,251,0.000354,1,
nlops-responder,gpt-5.2,openai,fast,1,4459,425,237,0.017040,1,
nlops-responder,qwen3-235b,qwen,best,1,6349,457,276,0.003665,1,
nlops-responder,gpt-5.2-pro,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.UnsupportedParamsError: gpt-5 models (including gpt-5-codex) don't support temperature=0.2. Only temperature=1 is supported. For gpt-5.1, temperature is supported when reasoning_effort='none' (or not specified, as it defaults to 'none'). To drop unsupported params set `litellm.drop_params = True`. Received Model Group=gpt-5.2-pro\nAvailable Model Group Fallbacks=None"",""type"":""None"",""param"":null,""code"":""400""}}"
nlops-responder,qwen3-80b-next,qwen,best,1,1890,457,208,0.002182,1,
nlops-responder,gpt-5.2-codex,openai,best,1,0,0,0,0.000000,0,"OpenAI API error 400: {""error"":{""message"":""litellm.BadRequestError: OpenAIException - {\n  \""error\"": {\n    \""message\"": \""Unsupported parameter: 'temperature' is not supported with this model.\"",\n    \""type\"": \""invalid_request_error\"",\n    \""param\"": \""temperature\"",\n    \""code\"": null\n  }\n}. Received Model Group=gpt-5.2-codex\nAvailable Model Group Fallbacks=None"",""type"":null,""param"":null,""code"":""400""}}"